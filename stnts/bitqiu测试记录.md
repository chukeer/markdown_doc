#BitQiu测试记录
##读写选举测试
###测试点
* 写一个文件，该文件的所有page分布在不同节点，剩余空间比例大的节点被选举次数更多
* 读一个文件，客户端从同集群或room的节点读取page，每个节点读取次数均匀

###写文件选举测试  
写一个1G的文件，page大小1M，4副本，统计如下两种数据

* 写之前节点磁盘剩余空间，查看日志找到节点状态日志，假设为725行到734行，统计脚本如下
	
		sed -n '725,734p' log/rootServer.INFO  | awk '{if (NF>=2){xx=split($2,x,":");yy=split($3,y,":");print y[2],"|",x[2]}}' | sort | awk '{print NR, "|", $0}'

* 每个page的选举情况。可以通过查看redis元数据统计，比如查看文件id12419的page分布情况，如下

		redis-cli hgetall FP12419 | tr ';' '\n' | awk -F "," '{if(NF>1)print $0}'  | sort | uniq -c
	  
client NodeId为`3,0,1,1,5,2,2`  
page数量为1000个，被选举次数一共为4000次，符合预期  
最后汇总数据得到如下表格

ID | NodeId | 磁盘使用率（%）  | 被选举次数
:---: | :---: | :---: | :---:
1 | 2,0,0,1,2,4,5,6 | 77.564453 | 503
2 | 2,0,0,1,3,4,5,6 | 70.377703 | 499
3 | 2,0,0,1,4,4,5,6 | 69.501567 | 499
4 | 2,0,0,1,5,4,5,6 | 69.503381 | 499 
5 | 2,0,0,2,2,4,5,6 | 95.212849 | 512
6 | 2,0,0,2,3,4,5,6 | 94.446692 | 523
7 | 2,0,0,2,6,4,5,6 | 79.882169 | 975

NodeId含义为`xx, xx ,xx, cluster_id, room_id, rack_id, host_id, service_id`，写4副本，每个cluster_id各写2个，不同cluster_id的节点里必须至少有两个节点room_id相同以便能进行不同cluster的forward操作，我们称这种节点为transfer节点  

1-4号节点cluster_id相同，每个page有2个副本落在这个集群，2-4号节点磁盘使用率接近，被选举次数接近，1号节点虽然磁盘使用率较高，但该节点和2号节点同为transfer节点，每次至少有一个随机被选举，所以选举次数也接近，符合预期  
5-7号节点cluster_id相同，每个page有2个副本落在这个集群，7号节点剩余空间较大，所以被选举次数较多，符合预期

根据上面的方法考察其它异常情况的选举

**某节点超负荷**  
在选举模块里规定，当节点磁盘利用率超过99%时将不会选举该节点，统计数据如下

ID | NodeId | 磁盘使用率（%）  | 被选举次数
:---: | :---: | :---: | :---:
1 | 2,0,0,1,2,4,5,6 | 78.570333 | 525
2 | 2,0,0,1,3,4,5,6 | 71.374887 | 536
3 | 2,0,0,1,4,4,5,6 | 99.667822 | 0
4 | 2,0,0,1,5,4,5,6 | 70.501037 | 939
5 | 2,0,0,2,2,4,5,6 | 96.234456 | 525
6 | 2,0,0,2,3,4,5,6 | 95.470151 | 500
7 | 2,0,0,2,6,4,5,6 | 81.831573 | 975

3号节点磁盘利用率超过99%，没有被选举，符合预期

**某集群符合要求的节点数不够**  
cluster_id为2的节点数只有1个，少于需要的2个

ID | NodeId | 磁盘使用率（%）  
:---: | :---: | :---: 
1 | 2,0,0,1,2,4,5,6 | 79.616332
2 | 2,0,0,1,3,4,5,6 | 72.443457
3 | 2,0,0,1,4,4,5,6 | 99.668528
4 | 2,0,0,1,5,4,5,6 | 72.372411
5 | 2,0,0,2,6,4,5,6 | 83.778698

结果：写文件选举失败，符合预期


###读文件选举测试  
准确来说读文件时没有节点的选举，在查询到每个page的节点存储情况时，要根据client的NodeId对这些节点排序，和client网络距离最近的放在列表的最前面，以便客户端拿到的节点列表里第一个总是最优的，这里主要考察这个排序的结果

读一个1G的文件，这里为/<12419>，通过统计客户端日志得出读取每个节点的次数

	grep "read page, dest_ip" log/rw_demo.INFO | awk -F ":" '{print $NF}' | sort | uniq -c
	
客户端NodeId`3,0,1,1,5,2,2`，读请求分布如下

ID | NodeId | 读取次数
:--: | :--: | :--:
1 | 2,0,0,1,2,4,5,6 | 205
2 | 2,0,0,1,3,4,5,6 | 209
3 | 2,0,0,1,4,4,5,6 | 295
4 | 2,0,0,1,5,4,5,6 | 291

读取次数一共为1000，由于每个节点读取优先级相同，读取请求并没有集中在某个节点上，符合预期。读取优先级按如下定义：

	1. 优先读取room_id相同的节点
	2. 如room_id相同，优先读取cluster_id相同的节点
	3. 如前两者相同，优先读取rack_id相同的节点，以此类推
	主要关注room_id和cluster_id
	
修改客户端NodeId，使其与某个节点在相同room_id，重新测试  
client NodeId`3,0,1,2,5,2,2`，读请求分布如下

ID | NodeId | 读取次数
:--: | :--: | :--:
1 | 2,0,0,1,2,4,5,6 | 503
2 | 2,0,0,1,3,4,5,6 | 228
3 | 2,0,0,1,4,4,5,6 | 128
4 | 2,0,0,1,5,4,5,6 | 130
5 | 2,0,0,2,2,4,5,6 | 11

本次client的room_id和1、5号节点相同，根据前面对读取优先级的定义，这些节点的优先级排序为`1, 5, (2, 3, 4)`，括号里的节点优先级相同，从前面的写测试可以看到，1号节点共写了503个page，所以在读这些page时请求全部落在1号节点上，5号节点共写了512个page，猜测和节点1重合了503个，所以剩余的11个page读请求全部落在5号节点上，其它读请求则随机选择节点，并没有集中落在某个节点上。结果符合预期

##UpdateFile
###测试方法

* 写一个本地文件到btq
* 修改本地文件
* 将修改后的文件update到btq
* 将update后的文件读回来和本地修改后的的比较，若相同则测试通过

###测试准备
####具有update功能的客户端
在原有测试客户端的基础上实现update功能，提供如下调用方式

	./rw_demo update btq_file local_file
该命令可以将btq_file更新为local_file，客户端会先将btq_file读回本地，比较其和local_file的差异，调用update_file协议，最后在将有差异的page写入对应的storeServer

####封装update客户端完成测试用例的脚本  

需要实现如下功能：  

1. 生成指定大小文件  
2. 修改指定部位内容  
3. 调用客户端实现写文件、更新文件、读文件操作

###测试观察点
每个测试用例执行前面测试方法列出的整个流程，每个测试用例修改文件的不同部位，包括以下几个

1. 修改文件开头部位
2. 修改文件中间部位
3. 修改文件末尾部位
4. 追加文件
5. 修改文件开头部位，并截断末尾部位

###测试结果
5个测试用例均通过，修改后文件和本地一致

###测试发现的问题
UpdateFileHandler里获取文件版本时出错，已修复